{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "No module named 'matplotlib'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3128176d115a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Simple neural network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Simple neural network. \n",
    "#       * single hidden layer with tanh function \n",
    "#       * identity function at the output layer\n",
    "#       * a squared error loss. \n",
    "#       * 30 hidden neurons (i.e. M=30) and 1 output neuron (i.e. K=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W1, W2):\n",
    "    '''\n",
    "        Inputs:\n",
    "        NxD matrix X of features, where N is the number of samples and D is the number of feature dimensions,\n",
    "        MxD matrix W1 of weights between the first and second layer of the network, where M is the number of hidden neurons, and\n",
    "        1xM matrix W2 of weights between the second and third layer of the network, where there is a single neuron at the output layer\n",
    "\n",
    "        Outputs:\n",
    "        Nx1 vector y_pred containing the outputs at the last layer for all N samples, and\n",
    "        NxM matrix Z containing the activations for all M hidden neurons of all N samples.\n",
    "    '''\n",
    "    a0 = np.dot(W1, X.T)      # hidden layer\n",
    "    z0 = np.tanh(a0.T)        # activation\n",
    "    a1 = np.dot(z0, W2.T)     # output layer\n",
    "\n",
    "    return z0, a1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(X, y, M, iters, eta):\n",
    "    '''\n",
    "        Performs training using backpropagation (and calls the forward function as it iterates). \n",
    "        Construct the network in this function --> create weight matrices + initialize to small random numbers\n",
    "        iterate: pick a training sample, compute the error at the output, backpropagate to the hidden layer\n",
    "        update the weights with the resulting error.\n",
    "\n",
    "        Inputs:\n",
    "        NxD matrix X of features, N = number of samples and D = number of feature dimensions, an Nx1 vector y containing the ground-truth labels for the N samples,\n",
    "        a scalar M containing the number of hidden neurons to use, a scalar iters defining how many iterations to run (one sample used in each), and a scalar eta defining the learning rate to use.\n",
    "        \n",
    "        Outputs:\n",
    "        W1 and W2, defined as above for forward, and\n",
    "        itersx1 vector error_over_time that contains the error on the sample used in each iteration.\n",
    "    '''\n",
    "\n",
    "    # initiaize wieghts\n",
    "    W1 = np.random.randn(M, 11) * 0.01\n",
    "    W2 = np.random.randn(1, M)  * 0.01 \n",
    "    error_over_time = []\n",
    "\n",
    "    # train network\n",
    "    for iter in range(iters): \n",
    "\n",
    "        i = np.random.randint(0,800)                    # take random sample \n",
    "        Xi = X[i,0:11].reshape(1,11)                    # extract feature dimensions\n",
    "        \n",
    "        Z, y_pred = forward(Xi, W1, W2)                 # forward propagation\n",
    "        \n",
    "        o_error =  y_pred - y[i, 0]                     # calcuate error of o/p units\n",
    "        W2 -= eta * Z * o_error                         # update wieghts onto o/p  layer\n",
    "\n",
    "        h_error = (1 - np.tanh(Z)**2) * o_error * W2    # calcuate error of hidden units \n",
    "        W1 -= eta * np.dot(Xi.T, h_error).T             # update error of hidden units\n",
    "\n",
    "        error_over_time.append(abs(np.asscalar(np.sqrt(np.mean(np.square(y_pred - y[i, 0]))))))\n",
    "    \n",
    "    return W1, W2, error_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'backprop' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b9e6eff018a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_over_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'backprop' is not defined"
     ]
    }
   ],
   "source": [
    "w1, w2, error_over_time = backprop(X = X_train, y = y_train, M = 30, iters = 1000, eta = 0.025)\n",
    "Z, y_test_pred = forward(X_test, w1, w2)\n",
    "print(np.sqrt(np.mean(np.square(y_test_pred - y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "figure = plt.scatter(list(range(0, 1000)), error_over_time)\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('Error (abs) ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "data = genfromtxt('winequality-red.csv', delimiter=';')\n",
    "\n",
    "# Standardize the data\n",
    "for i in range(11):\n",
    "    data[:, i] = (data[:, i] - np.mean(data[1:800, i])) / np.std(data[1:800, i]) + 1\n",
    "\n",
    "# split test / train\n",
    "X_train = data[1:801, 0:11]\n",
    "y_train = data[1:801,[11]]\n",
    "X_test = data[800:1600, 0:11]\n",
    "y_test = data[800:1600, [11]]"
   ]
  }
 ]
}